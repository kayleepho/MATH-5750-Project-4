{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kayleepho/MATH-5750-Project-4/blob/main/project4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Math 5750/6880: Mathematics of Data Science \\\n",
        "Project 4"
      ],
      "metadata": {
        "id": "ucxGNyGI_vs7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Exploratory Analysis\n",
        "\n",
        "Use the following code to download the dataset from\n",
        "[https://www.kaggle.com/code/mineshjethva/eda-pulsedb/notebook](https://www.kaggle.com/code/mineshjethva/eda-pulsedb/notebook). The dataset is described in the paper [https://doi.org/10.3389/fdgth.2022.1090854](https://doi.org/10.3389/fdgth.2022.1090854).\n",
        "\n",
        "I would recommend saving the data files to a google drive (or your local machine) so that you don't have to download them again. Note that the 5 data files correspond to the 5 columns in Table 4 of the paper.\n"
      ],
      "metadata": {
        "id": "nWcBGhqY_a1I"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-4AP1y_pxclv",
        "outputId": "b4bc73a3-e1de-47ef-a107-00be2c25ee3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/weinanwangrutgers/pulsedb-balanced-training-and-testing?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 17.3G/17.3G [07:20<00:00, 42.2MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/weinanwangrutgers/pulsedb-balanced-training-and-testing/versions/4\n"
          ]
        }
      ],
      "source": [
        "# download the data from kagglehub\n",
        "# The dataset is 17.3 G\n",
        "# This took about 15min using university wifi and, if\n",
        "# you save the data, you should only have to do it once\n",
        "\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"weinanwangrutgers/pulsedb-balanced-training-and-testing\")\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this block to move the data to a permanent directory in your drive\n",
        "\n",
        "import os, glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"/content/drive/MyDrive/pulsedb/\"\n",
        "!mkdir -p $DATA_DIR\n",
        "!cp -r $path/* $DATA_DIR"
      ],
      "metadata": {
        "id": "RnU6-XNKBpoz",
        "outputId": "70dd378f-90fa-4466-efe7-0645da797f5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# run this block after data is saved to your drive\n",
        "\n",
        "import os, glob\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "DATA_DIR = \"/content/drive/MyDrive/pulsedb/\"\n",
        "\n",
        "mat_files = sorted(glob.glob(os.path.join(DATA_DIR, \"**\", \"*.mat\"), recursive=True))\n",
        "print(f\"Found {len(mat_files)} .mat files\")\n",
        "for f in mat_files:\n",
        "    print(\" -\", f)"
      ],
      "metadata": {
        "id": "Cb3z3VM-xd1s",
        "outputId": "1fd72af9-abdd-4cbb-c261-96244f9ae439",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Found 5 .mat files\n",
            " - /content/drive/MyDrive/pulsedb/VitalDB_AAMI_Cal_Subset.mat\n",
            " - /content/drive/MyDrive/pulsedb/VitalDB_AAMI_Test_Subset.mat\n",
            " - /content/drive/MyDrive/pulsedb/VitalDB_CalBased_Test_Subset.mat\n",
            " - /content/drive/MyDrive/pulsedb/VitalDB_CalFree_Test_Subset.mat\n",
            " - /content/drive/MyDrive/pulsedb/VitalDB_Train_Subset.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll load the data. The data is about 20GB, which exceeds the colab basic RAM allocation. You can check your RAM using\n",
        "\n",
        "`!cat /proc/meminfo`\n",
        "\n",
        "You should upgrade to colab pro, which is free for students.\n",
        "\n",
        "[https://colab.research.google.com/signup](https://colab.research.google.com/signup)\n",
        "\n",
        "Then in 'change runtime type' click A100 GPU and high RAM."
      ],
      "metadata": {
        "id": "fybIsP4zmCeV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the data\n",
        "# the subject information is stored in a pandas df\n",
        "# the Signals (ECG, PPG, ABP) are stored in numpy arrays\n",
        "# this block takes 11 minutes to execute\n",
        "\n",
        "!cat /proc/meminfo\n",
        "\n",
        "!pip install mat73\n",
        "import mat73\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def load_mat_file(file_path):\n",
        "    data_dict = mat73.loadmat(file_path)['Subset']\n",
        "    print('finished loading'+file_path)\n",
        "    # print(data_dict.keys())\n",
        "\n",
        "    # first handle Signals\n",
        "    ECG = data_dict['Signals'][:,0,:]\n",
        "    PPG = data_dict['Signals'][:,1,:]\n",
        "    ABP = data_dict['Signals'][:,2,:]\n",
        "    data_dict.pop(\"Signals\", None)\n",
        "\n",
        "    data_dict['Age'] = data_dict['Age'].tolist()\n",
        "    data_dict['BMI'] = data_dict['BMI'].tolist()\n",
        "    data_dict['DBP'] = data_dict['DBP'].tolist()\n",
        "    data_dict['Gender'] = [1 if x[0] == 'M' else 0 for x in data_dict['Gender']]\n",
        "    data_dict['Height'] = data_dict['Height'].tolist()\n",
        "    data_dict['SBP'] = data_dict['SBP'].tolist()\n",
        "    data_dict['Subject'] = [x[0] for x in data_dict['Subject']]\n",
        "    data_dict['Weight'] = data_dict['Weight'].tolist()\n",
        "\n",
        "    data_df = pd.DataFrame(data_dict)\n",
        "    print('constructed df')\n",
        "\n",
        "    return data_df, ECG, PPG, ABP\n",
        "\n",
        "df_CalBased_Test, ECG_CalBased_Test, PPG_CalBased_Test, ABP_CalBased_Test = load_mat_file(DATA_DIR+'VitalDB_CalBased_Test_Subset.mat')\n",
        "df_Train, ECG_Train, PPG_Train, ABP_Train = load_mat_file(DATA_DIR+'VitalDB_Train_Subset.mat')"
      ],
      "metadata": {
        "id": "pSQFg0mK8cgd",
        "outputId": "1eae459f-50cd-41f4-a262-49e5e70e3302",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MemTotal:       13286956 kB\n",
            "MemFree:        11500484 kB\n",
            "MemAvailable:   12131180 kB\n",
            "Buffers:           59756 kB\n",
            "Cached:           626640 kB\n",
            "SwapCached:            0 kB\n",
            "Active:           363808 kB\n",
            "Inactive:         920588 kB\n",
            "Active(anon):       1604 kB\n",
            "Inactive(anon):   598508 kB\n",
            "Active(file):     362204 kB\n",
            "Inactive(file):   322080 kB\n",
            "Unevictable:          20 kB\n",
            "Mlocked:              20 kB\n",
            "SwapTotal:             0 kB\n",
            "SwapFree:              0 kB\n",
            "Dirty:               992 kB\n",
            "Writeback:             0 kB\n",
            "AnonPages:        598012 kB\n",
            "Mapped:           246328 kB\n",
            "Shmem:              2100 kB\n",
            "KReclaimable:     272236 kB\n",
            "Slab:             328568 kB\n",
            "SReclaimable:     272236 kB\n",
            "SUnreclaim:        56332 kB\n",
            "KernelStack:        7080 kB\n",
            "PageTables:         9592 kB\n",
            "SecPageTables:         0 kB\n",
            "NFS_Unstable:          0 kB\n",
            "Bounce:                0 kB\n",
            "WritebackTmp:          0 kB\n",
            "CommitLimit:     6643476 kB\n",
            "Committed_AS:    3440832 kB\n",
            "VmallocTotal:   34359738367 kB\n",
            "VmallocUsed:       95924 kB\n",
            "VmallocChunk:          0 kB\n",
            "Percpu:             1096 kB\n",
            "HardwareCorrupted:     0 kB\n",
            "AnonHugePages:      8192 kB\n",
            "ShmemHugePages:        0 kB\n",
            "ShmemPmdMapped:        0 kB\n",
            "FileHugePages:         0 kB\n",
            "FilePmdMapped:         0 kB\n",
            "CmaTotal:              0 kB\n",
            "CmaFree:               0 kB\n",
            "Unaccepted:            0 kB\n",
            "HugePages_Total:       0\n",
            "HugePages_Free:        0\n",
            "HugePages_Rsvd:        0\n",
            "HugePages_Surp:        0\n",
            "Hugepagesize:       2048 kB\n",
            "Hugetlb:               0 kB\n",
            "DirectMap4k:      193328 kB\n",
            "DirectMap2M:     8192000 kB\n",
            "DirectMap1G:     7340032 kB\n",
            "Requirement already satisfied: mat73 in /usr/local/lib/python3.12/dist-packages (0.65)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from mat73) (3.15.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from mat73) (2.0.2)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'DATA_DIR' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-4227978474.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mECG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPPG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mdf_CalBased_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mECG_CalBased_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPPG_CalBased_Test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABP_CalBased_Test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'VitalDB_CalBased_Test_Subset.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mdf_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mECG_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPPG_Train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABP_Train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_mat_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'VitalDB_Train_Subset.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'DATA_DIR' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df_CalBased_Test has 51720 entries\n",
        "print(df_CalBased_Test.keys())\n",
        "print(df_CalBased_Test.info())\n",
        "print(df_CalBased_Test.describe())\n",
        "df_CalBased_Test"
      ],
      "metadata": {
        "id": "GfBz5kcvEPRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1293 subjects, 40 samples/ subject = 51720 samples\n",
        "df_CalBased_Test['Subject'].value_counts()"
      ],
      "metadata": {
        "id": "xP89OfDQEqwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PulseDB dataset is a large, cleaned dataset based on MIMIC-III\n",
        "and VitalDB for benchmarking cuff-less blood pressure estimation methods1.\n",
        "- Read the paper to understand the dataset. The full dataset is available here: https://github.com/pulselabteam/PulseDB\n",
        "and a subset is here: https://www.kaggle.com/code/mineshjethva/eda-pulsedb\n",
        "- Use the provided code to import the PulseDB dataset.\n",
        "- Conduct an exploratory analysis."
      ],
      "metadata": {
        "id": "A12I6rMuWrc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "jIc-z3zuJWlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Blood Pressure Prediction\n",
        "\n",
        "Using the PulseDB dataset, we will build models to predict\n",
        "arterial blood pressure (ABP) from raw electrocardiogram (ECG) and photoplethysmogram (PPG) signals.\n",
        "- First, develop regression models that take ECG and PPG siganls and predict diastolic blood pressure (DBP) and systolic blood pressure (SBP).\n",
        "- First try a linear model and then try models of increasing complexity, e.g., fully connected NN, RNN, LSTM, transformer etc. Use the provided train/test data split.\n",
        "- Challenge problem: Develop models for the sequence-to-sequence prediction problem of predicting the full ABP waveform from ECG and PPG signals.\n",
        "- Again, first try a linear model to establish a baseline and then increase complexity by considering, e.g., fully connected NN, RNN,\n",
        "LSTM, transformer etc\n"
      ],
      "metadata": {
        "id": "rhZ5XU2rIpLU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "Bl2N87kvImVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Generative Modeling\n",
        "\n",
        "Using the PulseDB dataset, we will develop generative models for\n",
        "arterial blood pressure (ABP) signals.\n",
        "- First, perform a Principle Component Analysis (PCA) to understand the dimensionality of the ABP signals. Perform an “elbow analysis” to determine the intrinsic linear dimension of the data.\n",
        "- Then, train a fully connected or 1D convolutional autoencoder (AE) to minimize reconstruction loss (MSE between reconstructed and true ABP). Again, perform an elbow analysis to choose a latent dimension. - Compare this dimension to the dimension obtained via the PCA analysis. Use the decoder to generate some new ABP signals. Include plots with examples of both data and generated signals in your report."
      ],
      "metadata": {
        "id": "l0Qg1Sm8JP2p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "dEoQzvV4JPFI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}